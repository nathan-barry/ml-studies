{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv = \"./data/Cornell Movie Dataset/movie_conversations.txt\"\n",
    "corpus_movie_lines = \"./data/Cornell Movie Dataset/movie_lines.txt\"\n",
    "max_sentence_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_conv, 'r') as c:\n",
    "    conv = c.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_lines, 'r', encoding=\"latin1\") as l:\n",
    "    lines = l.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045', 'u0', 'm0', 'BIANCA', 'They do not!\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0].split(\" +++$+++ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict = {}\n",
    "for line in lines:\n",
    "    objects = line.split( ' +++$+++ ')\n",
    "    lines_dict[objects[0]] = objects[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(string):\n",
    "    punctuations = '''!()-[]{};:\\,{}./'@#$%^&*_~\"'''\n",
    "    no_punctuation = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punctuation += char\n",
    "    return no_punctuation.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "for con in conv:\n",
    "    ids = eval(con.split(' +++$+++ ')[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "\n",
    "        if i == len(ids) - 1:\n",
    "            break\n",
    "\n",
    "        first = remove_punc(lines_dict[ids[i]].strip())\n",
    "        second = remove_punc(lines_dict[ids[i+1]].strip())\n",
    "\n",
    "        qa_pairs.append(first.split()[:max_sentence_len])\n",
    "        qa_pairs.append(second.split()[:max_sentence_len])\n",
    "\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_freq = 5\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are 20079\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total words are {len(word_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/WORDMAP_corpus.json', 'w') as j:\n",
    "    json.dump(word_map, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(words, word_map):\n",
    "    return [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_sentence_len - len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_reply(words, word_map):\n",
    "    return [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_sentence_len - len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    question = encode_question(pair[0], word_map)\n",
    "    answer = encode_reply(pair[1], word_map)\n",
    "    pairs_encoded.append([question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/pairs_encoded.json', 'w') as j:\n",
    "    json.dump(pairs_encoded, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pairs = json.load(open('./data/pairs_encoded.json'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "            \n",
    "        return question, reply\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "\n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "\n",
    "    question_mask = (question != 0).to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    reply_input_mask = reply_input != 0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data)\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_target_mask = reply_target != 0\n",
    "\n",
    "    return question_mask, reply_input_mask, reply_target_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embeddings(vocab_size, d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0765b9855a7659208f612bcb0b9b5e473e80eecc03acde40f135996d767af958"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
